{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from dataset.ImageDatasetGenerator import ImageDatasetGenerator\n",
    "from dataset.generate_dataset import GenerateDataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from flip_gradient import flip_gradient\n",
    "from utils import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing source and target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset_path = 'C:\\\\Users\\\\calvom\\\\ThesisMaster\\\\barknet_1.0'\n",
    "target_dataset_path = 'C:\\\\Users\\\\calvom\\\\ThesisMaster\\\\notebooks\\\\downloaded_data\\\\full_data_top_23_0.1_resize_672_672'\n",
    "\n",
    "with open('C:\\\\Users\\\\calvom\\\\ThesisMaster\\\\barknet.metadata') as json_file:\n",
    "        source_dataset_metadata = json.load(json_file)\n",
    "\n",
    "with open('C:\\\\Users\\\\calvom\\\\ThesisMaster\\\\bdfr.metadata') as json_file:\n",
    "        target_dataset_metadata = json.load(json_file)\n",
    "        \n",
    "source_generate_dataset = GenerateDataset(source_dataset_path, source_dataset_metadata)\n",
    "target_generate_dataset = GenerateDataset(target_dataset_path, target_dataset_metadata)\n",
    "\n",
    "source_generate_dataset.load_dataset('None')\n",
    "target_generate_dataset.load_dataset('None')\n",
    "\n",
    "source_dataset_splits = source_generate_dataset.all_dataset()\n",
    "target_dataset_splits = target_generate_dataset.all_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_data = source_dataset_splits['train']\n",
    "source_val_data = source_dataset_splits['val']\n",
    "source_test_data = source_dataset_splits['test']\n",
    "\n",
    "target_train_data = target_dataset_splits['train']\n",
    "target_val_data = target_dataset_splits['val']\n",
    "target_test_data = target_dataset_splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "output_dim = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_params = {'vertical_flip': True, 'rotation': True, 'channel_shift_range': 10, 'shear_range': 0.2,\n",
    "                           'horizontal_flip': True, 'random_crop': True}\n",
    "\n",
    "source_train_image_generator = ImageDatasetGenerator(source_train_data['files'], source_train_data['labels'], batch_size//2, output_dim,\n",
    "                                        source_dataset_metadata, train_mode=True, **augmentation_params)\n",
    "\n",
    "source_val_image_generator = ImageDatasetGenerator(source_val_data['files'], source_val_data['labels'], batch_size//2, output_dim,\n",
    "                                        source_dataset_metadata, train_mode=False, random_crop=False)\n",
    "\n",
    "target_train_image_generator = ImageDatasetGenerator(target_train_data['files'], target_train_data['labels'], batch_size//2, output_dim,\n",
    "                                        target_dataset_metadata, train_mode=True, **augmentation_params)\n",
    "\n",
    "target_val_image_generator = ImageDatasetGenerator(target_val_data['files'], target_val_data['labels'], batch_size//2, output_dim,\n",
    "                                        target_dataset_metadata, train_mode=False, random_crop=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ds_train = tf.data.Dataset.from_generator(lambda: source_train_image_generator,\n",
    "                     output_types=(tf.float32, tf.float32),\n",
    "                     output_shapes=([batch_size//2, output_dim[0], output_dim[1], 3],\n",
    "                                    [batch_size//2, 20])\n",
    "                     )\n",
    "\n",
    "target_ds_train = tf.data.Dataset.from_generator(lambda: source_train_image_generator,\n",
    "                     output_types=(tf.float32, tf.float32),\n",
    "                     output_shapes=([batch_size//2, output_dim[0], output_dim[1], 3],\n",
    "                                    [batch_size//2, 20])\n",
    "                     )\n",
    "\n",
    "\n",
    "source_gen_train = iter(source_ds_train)\n",
    "target_gen_train = iter(target_ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, InputLayer, GlobalAveragePooling2D, Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class MNISTModel(object):\n",
    "    \"\"\"Simple MNIST domain adaptation model.\"\"\"\n",
    "    def __init__(self, extractor_model=None):\n",
    "        self.extractor_model = extractor_model\n",
    "        self._build_model()\n",
    "\n",
    "        \n",
    "    def _build_model(self):\n",
    "        input_shape = (224, 224, 3)\n",
    "        output_pooling = False\n",
    "        output_shape = 20\n",
    "        \n",
    "        #Feature extractor definition.\n",
    "        \n",
    "        input_features = Input(input_shape)\n",
    "        \n",
    "        if self.extractor_model is None:\n",
    "            output_1 = Conv2D(filters=32, kernel_size=5, padding='SAME')(input_features)\n",
    "            output_2 = MaxPooling2D(2, strides=2, padding='SAME')(output_1)\n",
    "            output_3 = Conv2D(filters=48, kernel_size=5, padding='SAME')(output_2)\n",
    "            output_4 = MaxPooling2D(2, strides=2, padding='SAME')(output_3)\n",
    "            if output_pooling:\n",
    "                final_output = GlobalAveragePooling2D()(output_4)\n",
    "            else:\n",
    "                final_output = Flatten()(output_4)\n",
    "        else:\n",
    "            \n",
    "            base_model = ResNet50(include_top=False, weights=None, input_shape=input_shape)\n",
    "            output_1 = preprocess_input(input_features)\n",
    "            output_2 = base_model(output_1, training=False)\n",
    "            final_output = GlobalAveragePooling2D()(output_2)\n",
    "            \n",
    "        self.feature_extractor_model = Model(inputs=input_features, outputs=final_output)\n",
    "        print(\"Feature extractor model summary: \")\n",
    "        self.feature_extractor_model.summary()\n",
    "        \n",
    "        #Source classifier definition.\n",
    "        source_output_1 = Dense(units=100)(final_output)\n",
    "        source_output_2 = Dense(units=100)(source_output_1)\n",
    "        source_final_output = Dense(units=output_shape, activation='softmax')(source_output_2)\n",
    "        self.source_classifier_model = Model(inputs=input_features, outputs=source_final_output)\n",
    "        print(\"Source classifier model summary: \")\n",
    "        print(self.source_classifier_model.summary())\n",
    "        \n",
    "        #Domain classifier definition.\n",
    "        #kernel_regularizer=tf.keras.regularizers.l2()\n",
    "        domain_output_1 = Dense(units=100)(final_output)\n",
    "        domain_final_output = Dense(units=1, \n",
    "                                    activation='sigmoid')(domain_output_1)\n",
    "        self.domain_classifier_model = Model(inputs=input_features, outputs=domain_final_output)\n",
    "        print(\"Domain classifier model summary: \")\n",
    "        print(self.domain_classifier_model.summary())\n",
    "        \n",
    "        #Combined model definition (source classifier + domain classifier).\n",
    "        self.combined_model = Model(inputs=input_features, outputs=[source_final_output, domain_final_output])\n",
    "        \n",
    "        print(\"Combined model summary: \")\n",
    "        print(self.combined_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTModel(extractor_model='resnet_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build optimizers\n",
    "from tensorflow.keras.optimizers import SGD, Adam,Adagrad\n",
    "from  tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy, BinaryAccuracy,SparseCategoricalAccuracy\n",
    "domain_optimizer = SGD(momentum=0.9)\n",
    "#domain_optimizer = Adam(1e-4)\n",
    "combined_model_optimizer = SGD(momentum=0.9)\n",
    "#combined_model_optimizer = Adam(1e-4)\n",
    "\n",
    "metrics=[\"accuracy\"]\n",
    "\n",
    "#model.domain_classifier_model.compile(loss='binary_crossentropy', optimizer=domain_optimizer, metrics=metrics)\n",
    "\n",
    "#model.combined_model.compile(loss=['sparse_categorical_crossentropy', 'binary_crossentropy'], \n",
    "#                             optimizer=combined_model_optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train_on_batch(X, y, model, optimizer, loss_function, metrics_function = None, trainable_variables=None):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(X)\n",
    "        loss = loss_function(y, pred)\n",
    "        metrics_results = {}\n",
    "        if metrics_function:\n",
    "            metrics_results = metrics_function(y, pred)\n",
    "    if not trainable_variables:\n",
    "        trainable_variables = model.trainable_variables\n",
    "    grads = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "    results = {'loss': loss}\n",
    "    results.update(metrics_results)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 8600\n",
    "\n",
    "domain_labels = np.vstack([np.tile([1.], [batch_size // 2, 1]),\n",
    "                                   np.tile([0.], [batch_size // 2, 1])])\n",
    "\n",
    "opposite_domain_labels = np.vstack([np.tile([0.], [batch_size // 2, 1]),\n",
    "                                   np.tile([1.], [batch_size // 2, 1])])\n",
    "\n",
    "sample_weights_class  = np.array(([1] * (batch_size // 2) + [0] * (batch_size // 2)))\n",
    "\n",
    "sample_weights_adversarial = np.ones((batch_size,))\n",
    "\n",
    "weights = [sample_weights_class, sample_weights_adversarial]\n",
    "    \n",
    "\n",
    "domain_binary_metric = BinaryAccuracy()\n",
    "source_classifier_metric = SparseCategoricalAccuracy()\n",
    "source_classifier_cum_loss = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "target_classifier_cum_loss = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "target_classifier_metric = SparseCategoricalAccuracy()\n",
    "\n",
    "def custom_loss_function(y_true, y_pred):\n",
    "    \n",
    "    sample_weight_class = weights[0]\n",
    "    sample_weight_domain = weights[1]\n",
    "    y_class = y_true[0]\n",
    "    y_domain = y_true[1]\n",
    "    y_pred_class = y_pred[0]\n",
    "    y_pred_domain = y_pred[1]\n",
    "    classifier_loss = SparseCategoricalCrossentropy()(y_class, y_pred_class, sample_weight=sample_weight_class)\n",
    "    domain_loss = BinaryCrossentropy()(y_domain, y_pred_domain, sample_weight=sample_weight_domain)\n",
    "    return classifier_loss + -1 * domain_loss\n",
    "\n",
    "\n",
    "def domain_metrics_functions(y, pred):\n",
    "    domain_binary_metric.update_state(y, pred)\n",
    "    result = domain_binary_metric.result()\n",
    "    return {domain_binary_metric.name: result}\n",
    "\n",
    "def combine_model_metrics_functions(y, pred):\n",
    "    source_classifier_metric.update_state(y[0], pred[0], weights[0])\n",
    "    source_result = source_classifier_metric.result()\n",
    "    \n",
    "    #target_classifier_metric.update_state(y[0], pred[0], 1 - weights[0])\n",
    "    #target_result = target_classifier_metric.result()\n",
    "    \n",
    "    source_classifier_cum_loss.update_state(y[0], pred[0], weights[0])\n",
    "    source_loss_result = source_classifier_cum_loss.result()\n",
    "    \n",
    "    #target_classifier_cum_loss.update_state(y[0], pred[0], 1 - weights[0])\n",
    "    #target_loss_result = target_classifier_cum_loss.result()\n",
    "            \n",
    "    return {f'source_{source_classifier_metric.name}': source_result, \n",
    "            #f'target_{target_classifier_metric.name}': target_result,\n",
    "           f'source_{source_classifier_cum_loss.name}': source_loss_result,}\n",
    "           #f'target_{target_classifier_cum_loss.name}': target_loss_result}\n",
    "    \n",
    "loss_print = 1\n",
    "\n",
    "#Training in adversarial manner.\n",
    "for i in range(num_steps):\n",
    "    \n",
    "    p = float(i) / num_steps\n",
    "    lr = 0.01 / (1. + 10 * p)**0.75\n",
    "    domain_optimizer.lr = lr   \n",
    "    combined_model_optimizer.lr = lr\n",
    "    \n",
    "    source_X, source_y = next(source_gen_train)\n",
    "    target_X, target_y = next(target_gen_train)\n",
    "\n",
    "    X = np.vstack([source_X, target_X])\n",
    "    y = np.concatenate([source_y, target_y])\n",
    "    \n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    #Training domain classifier.\n",
    "\n",
    "    domain_classifier_stats = custom_train_on_batch(X, \n",
    "                                                    domain_labels, \n",
    "                                                    model.domain_classifier_model,\n",
    "                                                    domain_optimizer,\n",
    "                                                    BinaryCrossentropy(),\n",
    "                                                    metrics_function=domain_metrics_functions)\n",
    "    \n",
    "    #Training source classifier and feature extractor together to fool domain classifier.\n",
    "    # We expect 0.5 accuracy for domain classifier.\n",
    "\n",
    "    labels = [y, domain_labels]\n",
    "    \n",
    "    combined_model_stats = custom_train_on_batch(X, \n",
    "                                                    labels, \n",
    "                                                    model.combined_model,\n",
    "                                                    combined_model_optimizer,\n",
    "                                                    custom_loss_function,\n",
    "                                                    metrics_function=combine_model_metrics_functions)\n",
    "    \n",
    "    if i % loss_print == 0:\n",
    "        print(combined_model_stats)\n",
    "        domain_loss = domain_classifier_stats['loss']\n",
    "        domain_accuracy = domain_classifier_stats['binary_accuracy']\n",
    "        combined_model_loss = combined_model_stats['loss']\n",
    "        source_accuracy = combined_model_stats['source_sparse_categorical_accuracy']\n",
    "        #target_accuracy = combined_model_stats['target_sparse_categorical_accuracy']\n",
    "        source_cum_loss = combined_model_stats['source_sparse_categorical_crossentropy']\n",
    "        #target_cum_loss = combined_model_stats['target_sparse_categorical_crossentropy']\n",
    "        print('---------------------------------------------------')\n",
    "        print(f'Iteration: {i}')\n",
    "        print(f'Domain Classifier Loss: {domain_loss}')\n",
    "        print(f'Domain Classifier Accuracy: {domain_accuracy}')\n",
    "        print(f'Combined model Loss: {combined_model_loss}')\n",
    "        print(f'Source Classifier Loss: {source_cum_loss}')\n",
    "        #print(f'Target Classifier Loss: {target_cum_loss}')\n",
    "        print(f'Source Accuracy: {source_accuracy}')\n",
    "        #print(f'Target Accuracy: {target_accuracy}')\n",
    "        print(f'Actual learning rate: {lr}')\n",
    "        print('---------------------------------------------------')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=3000)\n",
    "#dann_tsne = tsne.fit_transform(dann_emb)\n",
    "#dann_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_embedding(source_only_tsne, combined_test_labels.argmax(1), combined_test_domain.argmax(1), 'Source only')\n",
    "#plot_embedding(dann_tsne, combined_test_labels.flatten(), combined_test_domain[:, 0], 'Domain Adaptation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=3000)\n",
    "#source_only_tsne = tsne.fit_transform(source_only_emb)\n",
    "\n",
    "#tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=3000)\n",
    "#dann_tsne = tsne.fit_transform(dann_emb)\n",
    "        \n",
    "#plot_embedding(source_only_tsne, combined_test_labels.argmax(1), combined_test_domain.argmax(1), 'Source only')\n",
    "#plot_embedding(dann_tsne, combined_test_labels.argmax(1), combined_test_domain.argmax(1), 'Domain Adaptation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
